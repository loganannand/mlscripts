{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\lannand\\OneDrive - IB4T Ltd\\Desktop\\Py\\UML\\Machine Learning A-Z (Codes and Datasets)\\Part 1 - Data Preprocessing\\Section 2, Part 1 - Data Preprocessing\\Python\\Data.csv'\n",
    "dataset = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values # rows = \":\", i.e., no upper or lower limit ~ all rows. Columns are all BUT last, i.e., upper limit = -1\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') # Replace missing values with the mean of the rest\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"OneHotEncoder\" to encode countries into coloumns (as vectors)\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') # Parameters of 'transformers=' list: name (str), transformer (i.e., 'drop', 'passthrough' or 'estimator'), columns. \n",
    "# \"remainder=\" 'drop' or 'passthrough'\n",
    "\n",
    "X = np.array(ct.fit_transform(X)) # \"fit_transform\" applies the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() # Use label encoder to transform yes/no into binary 1/0\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply feature scaling AFTER splitting the test and trianing sets - this is to avoid \"information leakage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # random state there just for tutorial purposes - it keeps the splits the same each time this cell is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main two techniques are \"standardisation\" and \"normalisation\" - when to go for which?\n",
    "\n",
    "Normalisation is good when you have a normal distribution in your dataset. \n",
    "Standardisation works well all the time\n",
    "\n",
    "DO NOT apply feature scaling to your dummy variables, i.e., those created to incode \"country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling\n",
    "sc = StandardScaler()\n",
    "# dont include for dummy variables, i.e., those used for \"country\" column\n",
    "\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.fit_transform(X_test[:, 3:])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
